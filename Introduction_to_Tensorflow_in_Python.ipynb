{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMX3cVZ/NTLHkcDrQj9wDaF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mublify/Deep_Learning/blob/main/Introduction_to_Tensorflow_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to Tensorflow in Python"
      ],
      "metadata": {
        "id": "nzFtvm4_XsYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you have a box of chocolates. This box is a simple container, right? Now, imagine you have a multi-dimensional box, like a Rubik's Cube. This cube has layers, rows, and columns, and each little cube within it can hold a different piece of information.\n",
        "\n",
        "![Rubik's Cube](https://live.staticflickr.com/3691/11913436786_fa99d2f1bb_b.jpg)\n",
        "\n",
        "In the world of artificial intelligence, tensors are like these multi-dimensional boxes. They are used to store and process complex data, such as images, videos, and text. For example, an image can be represented as a tensor where each dimension represents the height, width, and color channels of the image.\n",
        "\n",
        "These tensors are the building blocks of many AI algorithms and models. They allow computers to understand and analyze complex patterns within data, which is crucial for tasks like image recognition, natural language processing, and even self-driving cars.\n",
        "\n",
        "![Artificial Intelligence](https://img.ccnull.de/1095000/preview/1099986_dc09d38e98721d53ec573553c3e1ff91.jpg)"
      ],
      "metadata": {
        "id": "j1PyYsUOXpDX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary Classification"
      ],
      "metadata": {
        "id": "zuTa4i_JmgJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting Bank Notes Data using genfromtxt library\n",
        "\n",
        "Think of numpy.genfromtxt as a handy tool that helps you tidy up messy data. It's like a magical wand that can transform a messy text file into a clean, organized NumPy array."
      ],
      "metadata": {
        "id": "BXC-PJrLoMMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import genfromtxt\n",
        "data=genfromtxt('bank_note_data.txt',delimiter =',')\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwdkLG7Xl77N",
        "outputId": "32af36f2-010a-4adc-8311-7d5397973f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  3.6216 ,   8.6661 ,  -2.8073 ,  -0.44699,   0.     ],\n",
              "       [  4.5459 ,   8.1674 ,  -2.4586 ,  -1.4621 ,   0.     ],\n",
              "       [  3.866  ,  -2.6383 ,   1.9242 ,   0.10645,   0.     ],\n",
              "       ...,\n",
              "       [ -3.7503 , -13.4586 ,  17.5932 ,  -2.7771 ,   1.     ],\n",
              "       [ -3.5637 ,  -8.3827 ,  12.393  ,  -1.2823 ,   1.     ],\n",
              "       [ -2.5419 ,  -0.65804,   2.6842 ,   1.1952 ,   1.     ]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Summary\n",
        "\n",
        "This dataset is a collection of images of genuine and counterfeit banknotes, processed to extract key features. It's commonly used for training and evaluating machine learning models for banknote authentication.\n",
        "\n",
        "#### Features:\n",
        "\n",
        "**Variance:** Measures image texture variation.\n",
        "\n",
        "**Skewness:** Quantifies image asymmetry.\n",
        "\n",
        "**Curtosis:** Captures image tailedness.\n",
        "\n",
        "**Entropy:** Reflects image randomness or information content.\n",
        "\n",
        "#### Labels:\n",
        "**Class:** Indicates whether the banknote is genuine (1) or counterfeit (0)."
      ],
      "metadata": {
        "id": "osgji0dgpOcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels=data[:,-1]\n",
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rlBDAlboaa3",
        "outputId": "65794ee2-8c49-4785-8210-3136532bcbf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., ..., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features=data[:,0:4]\n",
        "features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkz6ZuUFpons",
        "outputId": "45a70774-ab37-4b3d-8cb3-78c521146b8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  3.6216 ,   8.6661 ,  -2.8073 ,  -0.44699],\n",
              "       [  4.5459 ,   8.1674 ,  -2.4586 ,  -1.4621 ],\n",
              "       [  3.866  ,  -2.6383 ,   1.9242 ,   0.10645],\n",
              "       ...,\n",
              "       [ -3.7503 , -13.4586 ,  17.5932 ,  -2.7771 ],\n",
              "       [ -3.5637 ,  -8.3827 ,  12.393  ,  -1.2823 ],\n",
              "       [ -2.5419 ,  -0.65804,   2.6842 ,   1.1952 ]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=features\n",
        "y=labels"
      ],
      "metadata": {
        "id": "SVU90x07pvl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Test Split\n",
        "\n",
        "\n",
        "To prevent overfitting and ensure the model's ability to generalize to new, unseen data."
      ],
      "metadata": {
        "id": "2ZL1Y06Wp49q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=0)"
      ],
      "metadata": {
        "id": "ucP2ns8xp3hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standardization of Data\n",
        "\n",
        "To scale data to fit within a specific range, typically [0, 1]. It’s useful when features have varied ranges, and to avoid bias and simplifying compute."
      ],
      "metadata": {
        "id": "e1y_N_0xq8Ri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler=MinMaxScaler()\n",
        "\n",
        "#Identify the number\n",
        "scaler.fit(X_train)\n",
        "\n",
        "#Divide the number\n",
        "X_train_scaled=scaler.transform(X_train)"
      ],
      "metadata": {
        "id": "l--m3tyBq_IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_scaled=scaler.fit_transform(X_test)"
      ],
      "metadata": {
        "id": "BUQktPihSnQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Neural Network\n",
        "\n",
        "for learning complex, often a non-linear machine learning model."
      ],
      "metadata": {
        "id": "23JNieHVS0GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "metadata": {
        "id": "mQbuHIiUS3Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Activation Functions\n",
        "\n",
        "An activation function takes the input signals from the previous layer and converts them into an output signal that gets sent to the next layer in the network. [Reference](https://thedatainterview.substack.com/p/why-neural-networks-need-an-activation-function)"
      ],
      "metadata": {
        "id": "C3aJZhYuWOTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=Sequential()\n",
        "\n",
        "#First hidden layer\n",
        "model.add(Dense(5, input_dim=4, activation='relu')) #5-neurons, input_dim=4 (number of inputs), activation=function\n",
        "\n",
        "#Second hidden layer\n",
        "model.add(Dense(4, activation='relu'))\n",
        "\n",
        "#Output layer\n",
        "model.add(Dense(1, activation='sigmoid')) #Sigmoid Activation function is the best option for the output layer of Binary Classification. Not for others.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuU-DaaPUxg6",
        "outputId": "c62cb2e7-c06d-4d9c-a29f-bedefd72d9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compile Model\n",
        "\n",
        "Compile the model by specifying the optimizer, loss function, and evaluation metric. [Reference](https://medium.com/@sumit.kaul.87/building-deep-learning-models-with-keras-a-step-by-step-guide-with-code-examples-68aee4152625)\n",
        "\n",
        "**Loss Calculation:** The network’s output is compared to the actual answer, and a “loss” (error) is calculated, which reflects how far the prediction is from the target. [Reference](https://medium.com/@njorogeofrey73/a-gental-introduction-to-neural-networks-3ebda1284984)\n",
        "\n",
        "**Adam Optimizer:** The Adam optimizer combines the advantages of Momentum and RMSProp, enabling adaptive learning-rate adjustments and thereby improving convergence speed.[Reference](https://www.mdpi.com/2223-7747/13/22/3151)"
      ],
      "metadata": {
        "id": "U14qiCx2c_2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Q_czs3QRdf9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the model\n",
        "\n",
        "The number of **epochs** is a hyperparameter of gradient descent that controls the number of training samples to work through before the model’s internal parameters are updated. [Reference](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch)\n",
        "\n",
        "**Verbose** will bring the model summary at each iteration. [Reference](https://www.analyticsvidhya.com/blog/2021/04/forward-feature-selection-and-its-implementation/)"
      ],
      "metadata": {
        "id": "iG8pMSM9dyfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train_scaled,y_train, epochs=50, verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QUVrBrLud0gZ",
        "outputId": "92c5edbb-acc7-4cb2-80a3-9b17e59e0c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9856 - loss: 0.1630\n",
            "Epoch 2/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9753 - loss: 0.1639 \n",
            "Epoch 3/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9880 - loss: 0.1550\n",
            "Epoch 4/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9848 - loss: 0.1474 \n",
            "Epoch 5/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9822 - loss: 0.1457 \n",
            "Epoch 6/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9889 - loss: 0.1405 \n",
            "Epoch 7/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9877 - loss: 0.1319 \n",
            "Epoch 8/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.1347\n",
            "Epoch 9/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9853 - loss: 0.1371\n",
            "Epoch 10/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9838 - loss: 0.1283 \n",
            "Epoch 11/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9888 - loss: 0.1281\n",
            "Epoch 12/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.1306\n",
            "Epoch 13/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9877 - loss: 0.1180 \n",
            "Epoch 14/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9809 - loss: 0.1229\n",
            "Epoch 15/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9868 - loss: 0.1238\n",
            "Epoch 16/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9863 - loss: 0.1149 \n",
            "Epoch 17/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9870 - loss: 0.1144 \n",
            "Epoch 18/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9924 - loss: 0.1084 \n",
            "Epoch 19/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9938 - loss: 0.1081 \n",
            "Epoch 20/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9880 - loss: 0.1085\n",
            "Epoch 21/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9896 - loss: 0.1083\n",
            "Epoch 22/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9859 - loss: 0.1086 \n",
            "Epoch 23/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9909 - loss: 0.1019\n",
            "Epoch 24/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9846 - loss: 0.1023 \n",
            "Epoch 25/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9852 - loss: 0.0975\n",
            "Epoch 26/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9885 - loss: 0.0963\n",
            "Epoch 27/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9917 - loss: 0.0921 \n",
            "Epoch 28/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9879 - loss: 0.0954\n",
            "Epoch 29/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9768 - loss: 0.0980 \n",
            "Epoch 30/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9865 - loss: 0.0966 \n",
            "Epoch 31/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9790 - loss: 0.0948\n",
            "Epoch 32/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9858 - loss: 0.0887\n",
            "Epoch 33/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9841 - loss: 0.0875 \n",
            "Epoch 34/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9846 - loss: 0.0854 \n",
            "Epoch 35/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9860 - loss: 0.0902 \n",
            "Epoch 36/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9917 - loss: 0.0848 \n",
            "Epoch 37/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9899 - loss: 0.0787 \n",
            "Epoch 38/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0821\n",
            "Epoch 39/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0799\n",
            "Epoch 40/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9885 - loss: 0.0820\n",
            "Epoch 41/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0815\n",
            "Epoch 42/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9853 - loss: 0.0779 \n",
            "Epoch 43/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9862 - loss: 0.0774\n",
            "Epoch 44/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9791 - loss: 0.0827\n",
            "Epoch 45/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9865 - loss: 0.0775 \n",
            "Epoch 46/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9863 - loss: 0.0736 \n",
            "Epoch 47/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9896 - loss: 0.0709\n",
            "Epoch 48/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9852 - loss: 0.0793\n",
            "Epoch 49/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9882 - loss: 0.0720\n",
            "Epoch 50/50\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9796 - loss: 0.0795 \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f612d6849a0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions=model.predict(x_test_scaled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yECsPyaCiYbM",
        "outputId": "55a68592-6f96-4cb1-82cd-8f59db26ea9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NumPy’s where() function filters and indexes arrays. It takes a condition as input and returns a boolean array indicating which elements satisfy the condition. The function converts probabilities (in the predictions array) into binary classifications (in the predicted_classes array)."
      ],
      "metadata": {
        "id": "XT2ng39Oj5vT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes=np.where(predictions>0.5,1,0)\n"
      ],
      "metadata": {
        "id": "zV6376WUiYRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "metadata": {
        "id": "Q2inRiTtmAvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classification_report(y_test,predicted_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "z5IBa3XWmKPC",
        "outputId": "83133ed1-338e-4152-c164-0ad39712693e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'              precision    recall  f1-score   support\\n\\n         0.0       1.00      0.98      0.99       157\\n         1.0       0.98      1.00      0.99       118\\n\\n    accuracy                           0.99       275\\n   macro avg       0.99      0.99      0.99       275\\nweighted avg       0.99      0.99      0.99       275\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(y_test,predicted_classes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpveSSwmmP9V",
        "outputId": "03744420-cb5e-4e9f-d0c5-bbf4924cc1c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[154   3]\n",
            " [  0 118]]\n"
          ]
        }
      ]
    }
  ]
}